{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Занятие 6.1: Введение в обработку текста\n",
    "\n",
    "Максим Ионов, [max.ionov@gmail.com](mailto:max.ionov@gmail.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Чтение текстов, нужных для упражнений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "news = sklearn.datasets.fetch_20newsgroups(subset='train', categories=['comp.graphics', 'sci.med'], shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comp.graphics', 'sci.med']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'From: zyeh@caspian.usc.edu (zhenghao yeh)\\nSubject: Re: Need polygon splitting algo...\\nOrganization: University of Southern California, Los Angeles, CA\\nLines: 25\\nDistribution: world\\nNNTP-Posting-Host: caspian.usc.edu\\nKeywords: polygons, splitting, clipping\\n\\n\\nIn article <1qvq4b$r4t@wampyr.cc.uow.edu.au>, g9134255@wampyr.cc.uow.edu.au (Coronado Emmanuel Abad) writes:\\n|> \\n|> The idea is to clip one polygon using another polygon (not\\n|> necessarily rectangular) as a window.  My problem then is in\\n|> finding out all the new vertices of the resulting \"subpolygons\"\\n|> from the first one.  Is this simply a matter of extending the\\n|> usual algorithm whereby each of the edges of one polygon is checked\\n|> against another polygon???  Is there a simpler way??\\n|> \\n|> Comments welcome.\\n|> \\n|> Noel.\\n\\n\\tIt depends on what kind of the polygons. \\n\\tConvex - simple, concave - trouble, concave with loop(s)\\n\\tinside - big trouble.\\n\\n\\tOf cause, you can use the box test to avoid checking\\n\\teach edges. According to my experience, there is not\\n\\ta simple way to go. The headache stuff is to deal with\\n\\tthe special cases, for example, the overlapped lines.\\n\\n\\tYeh\\n\\tUSC\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Строим частотный список"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import collections\n",
    "\n",
    "def get_freq_list(texts):\n",
    "    words = collections.defaultdict(int)\n",
    "    for text in texts:\n",
    "        for word in re.split(r'\\W', text):\n",
    "            words[word] += 1\n",
    "    del words[u'']\n",
    "    \n",
    "    return [(word, words[word]) for word in sorted(words, key=lambda w: words[w], reverse=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bag of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.feature_extraction.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vctr = sklearn.feature_extraction.text.CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_counts = vctr.fit_transform(news.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vctr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vctr.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vctr.vocabulary_.get('the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. TF.IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vctr_tfidf = sklearn.feature_extraction.text.TfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_counts_tfidf = vctr_tfidf.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vctr_tfidf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vctr_tfidf.vocabulary_.get('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.pipeline\n",
    "import sklearn.naive_bayes\n",
    "\n",
    "text_clf = sklearn.pipeline.Pipeline([('vect', sklearn.feature_extraction.text.CountVectorizer()),\n",
    "                     ('clf', sklearn.naive_bayes.MultinomialNB())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Classifying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "news_test = sklearn.datasets.fetch_20newsgroups(subset='test',\n",
    "     categories=['comp.graphics', 'sci.med'], shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_clf.fit(news.data, news.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicted = text_clf.predict(news_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics\n",
    "print(sklearn.metrics.classification_report(news_test.target, predicted,\n",
    "     target_names=news_test.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sklearn.metrics.confusion_matrix(news_test.target, predicted)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
