{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Извлечение данных, генерация текстов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Извлечение ключевых слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. RAKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_en = \"\"\"\n",
    "Distributional hypothesis\n",
    "The distributional hypothesis in linguistics is derived from the semantic theory of language usage, \n",
    "i.e. words that are used and occur in the same contexts tend to purport similar meanings.\n",
    "The underlying idea that \"a word is characterized by the company it keeps\" was popularized by Firth. \n",
    "The Distributional Hypothesis is the basis for statistical semantics. Although the Distributional Hypothesis originated in linguistics,\n",
    "it is now receiving attention in cognitive science especially regarding the context of word use.\n",
    "In recent years, the distributional hypothesis has provided the basis for the theory of similarity-based generalization in language learning:\n",
    "the idea that children can figure out how to use words they've rarely encountered before by generalizing about their use from distributions of similar words.\n",
    "The distributional hypothesis suggests that the more semantically similar two words are, the more distributionally similar they will be in turn, and thus the more that they will tend to occur in similar linguistic contexts. \n",
    "Whether or not this suggestion holds has significant implications for both the data-sparsity problem in computational modeling, \n",
    "and for the question of how children are able to learn language so rapidly given relatively impoverished input \n",
    "(this is also known as the problem of the poverty of the stimulus).\n",
    "\n",
    "Distributional semantic modeling\n",
    "Distributional semantics favor the use of linear algebra as computational tool and representational framework. \n",
    "The basic approach is to collect distributional information in high-dimensional vectors, \n",
    "and to define distributional/semantic similarity in terms of vector similarity. Different kinds of similarities can be extracted \n",
    "depending on which type of distributional information is used to collect the vectors: topical similarities can be extracted by populating \n",
    "the vectors with information on which text regions the linguistic items occur in; paradigmatic similarities can be extracted by populating \n",
    "the vectors with information on which other linguistic items the items co-occur with. Note that the latter type of vectors can also be \n",
    "used to extract syntagmatic similarities by looking at the individual vector components.\n",
    "The basic idea of a correlation between distributional and semantic similarity can be operationalized in many different ways. \n",
    "There is a rich variety of computational models implementing distributional semantics, including latent semantic analysis (LSA),\n",
    "Hyperspace Analogue to Language (HAL), syntax- or dependency-based models, random indexing, semantic folding\n",
    "and various variants of the topic model.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rake_nltk\n",
      "Requirement already satisfied: nltk in /Users/max/anaconda/lib/python2.7/site-packages (from rake_nltk)\n",
      "Installing collected packages: rake-nltk\n",
      "Successfully installed rake-nltk-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install rake_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import rake_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = rake_nltk.Rake(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r.extract_keywords_from_text(text_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rapidly given relatively impoverished input',\n",
       " 'distributional semantic modeling distributional semantics favor',\n",
       " 'computational models implementing distributional semantics',\n",
       " 'cognitive science especially regarding',\n",
       " 'lsa ), hyperspace analogue',\n",
       " 'including latent semantic analysis',\n",
       " 'semantically similar two words',\n",
       " 'hal ), syntax',\n",
       " 'distributional hypothesis suggests',\n",
       " 'distributional hypothesis originated',\n",
       " 'purport similar meanings',\n",
       " 'many different ways',\n",
       " 'individual vector components',\n",
       " 'extract syntagmatic similarities',\n",
       " 'similar linguistic contexts',\n",
       " 'collect distributional information',\n",
       " 'linguistic items occur',\n",
       " 'computational modeling',\n",
       " 'statistical semantics',\n",
       " 'distributional hypothesis',\n",
       " 'based models',\n",
       " 'distributional information',\n",
       " 'define distributional',\n",
       " 'semantic folding',\n",
       " 'similar words',\n",
       " 'linguistic items',\n",
       " 'computational tool',\n",
       " 'semantic similarity',\n",
       " 'distributionally similar',\n",
       " 'semantic theory',\n",
       " 'different kinds',\n",
       " 'items co',\n",
       " 'vector similarity',\n",
       " 'various variants',\n",
       " 'topical similarities',\n",
       " 'topic model',\n",
       " 'text regions',\n",
       " 'suggestion holds',\n",
       " 'stimulus ).',\n",
       " 'significant implications',\n",
       " 'rich variety',\n",
       " 'representational framework',\n",
       " 'recent years',\n",
       " 'receiving attention',\n",
       " 'rarely encountered',\n",
       " 'random indexing',\n",
       " 'paradigmatic similarities',\n",
       " 'linear algebra',\n",
       " 'contexts tend',\n",
       " 'basic approach',\n",
       " 'based generalization',\n",
       " 'use words',\n",
       " 'learn language',\n",
       " 'language usage',\n",
       " 'language learning',\n",
       " 'underlying idea',\n",
       " 'basic idea',\n",
       " 'sparsity problem',\n",
       " 'latter type',\n",
       " 'extracted depending',\n",
       " 'dimensional vectors',\n",
       " 'also known',\n",
       " 'distributional',\n",
       " 'word use',\n",
       " 'words',\n",
       " 'similarities',\n",
       " 'occur',\n",
       " 'information',\n",
       " 'collect',\n",
       " 'language',\n",
       " 'use',\n",
       " 'similarity',\n",
       " 'idea',\n",
       " 'word',\n",
       " 'vectors',\n",
       " 'type',\n",
       " 'theory',\n",
       " 'tend',\n",
       " 'problem',\n",
       " 'extracted',\n",
       " 'also',\n",
       " 'whether',\n",
       " 'used',\n",
       " 'turn',\n",
       " 'thus',\n",
       " 'terms',\n",
       " 'question',\n",
       " 'provided',\n",
       " 'poverty',\n",
       " 'populating',\n",
       " 'popularized',\n",
       " 'operationalized',\n",
       " 'note',\n",
       " 'looking',\n",
       " 'linguistics',\n",
       " 'keeps',\n",
       " 'high',\n",
       " 'generalizing',\n",
       " 'firth',\n",
       " 'figure',\n",
       " 'e',\n",
       " 'distributions',\n",
       " 'derived',\n",
       " 'dependency',\n",
       " 'data',\n",
       " 'correlation',\n",
       " 'context',\n",
       " 'company',\n",
       " 'children',\n",
       " 'characterized',\n",
       " 'basis',\n",
       " 'although',\n",
       " 'able']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.get_ranked_phrases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(25.0, 'rapidly given relatively impoverished input'),\n",
       " (24.133333333333333,\n",
       "  'distributional semantic modeling distributional semantics favor'),\n",
       " (19.133333333333333,\n",
       "  'computational models implementing distributional semantics'),\n",
       " (16.0, 'cognitive science especially regarding'),\n",
       " (15.5, 'lsa ), hyperspace analogue'),\n",
       " (15.2, 'including latent semantic analysis'),\n",
       " (13.05, 'semantically similar two words'),\n",
       " (9.5, 'hal ), syntax'),\n",
       " (8.966666666666667, 'distributional hypothesis suggests'),\n",
       " (8.966666666666667, 'distributional hypothesis originated'),\n",
       " (8.8, 'purport similar meanings'),\n",
       " (8.5, 'many different ways'),\n",
       " (8.5, 'individual vector components'),\n",
       " (8.0, 'extract syntagmatic similarities'),\n",
       " (7.966666666666667, 'similar linguistic contexts'),\n",
       " (7.3, 'collect distributional information'),\n",
       " (7.0, 'linguistic items occur'),\n",
       " (7.0, 'computational modeling'),\n",
       " (6.333333333333333, 'statistical semantics'),\n",
       " (5.966666666666667, 'distributional hypothesis'),\n",
       " (5.5, 'based models'),\n",
       " (5.3, 'distributional information'),\n",
       " (5.3, 'define distributional'),\n",
       " (5.2, 'semantic folding'),\n",
       " (5.05, 'similar words'),\n",
       " (5.0, 'linguistic items'),\n",
       " (5.0, 'computational tool'),\n",
       " (4.866666666666667, 'semantic similarity'),\n",
       " (4.8, 'distributionally similar'),\n",
       " (4.7, 'semantic theory'),\n",
       " (4.5, 'different kinds'),\n",
       " (4.333333333333334, 'items co'),\n",
       " (4.166666666666667, 'vector similarity'),\n",
       " (4.0, 'various variants'),\n",
       " (4.0, 'topical similarities'),\n",
       " (4.0, 'topic model'),\n",
       " (4.0, 'text regions'),\n",
       " (4.0, 'suggestion holds'),\n",
       " (4.0, 'stimulus ).'),\n",
       " (4.0, 'significant implications'),\n",
       " (4.0, 'rich variety'),\n",
       " (4.0, 'representational framework'),\n",
       " (4.0, 'recent years'),\n",
       " (4.0, 'receiving attention'),\n",
       " (4.0, 'rarely encountered'),\n",
       " (4.0, 'random indexing'),\n",
       " (4.0, 'paradigmatic similarities'),\n",
       " (4.0, 'linear algebra'),\n",
       " (4.0, 'contexts tend'),\n",
       " (4.0, 'basic approach'),\n",
       " (4.0, 'based generalization'),\n",
       " (3.916666666666667, 'use words'),\n",
       " (3.75, 'learn language'),\n",
       " (3.75, 'language usage'),\n",
       " (3.75, 'language learning'),\n",
       " (3.666666666666667, 'underlying idea'),\n",
       " (3.666666666666667, 'basic idea'),\n",
       " (3.5, 'sparsity problem'),\n",
       " (3.5, 'latter type'),\n",
       " (3.5, 'extracted depending'),\n",
       " (3.5, 'dimensional vectors'),\n",
       " (3.5, 'also known'),\n",
       " (3.3, 'distributional'),\n",
       " (3.166666666666667, 'word use'),\n",
       " (2.25, 'words'),\n",
       " (2.0, 'similarities'),\n",
       " (2.0, 'occur'),\n",
       " (2.0, 'information'),\n",
       " (2.0, 'collect'),\n",
       " (1.75, 'language'),\n",
       " (1.6666666666666667, 'use'),\n",
       " (1.6666666666666667, 'similarity'),\n",
       " (1.6666666666666667, 'idea'),\n",
       " (1.5, 'word'),\n",
       " (1.5, 'vectors'),\n",
       " (1.5, 'type'),\n",
       " (1.5, 'theory'),\n",
       " (1.5, 'tend'),\n",
       " (1.5, 'problem'),\n",
       " (1.5, 'extracted'),\n",
       " (1.5, 'also'),\n",
       " (1.0, 'whether'),\n",
       " (1.0, 'used'),\n",
       " (1.0, 'turn'),\n",
       " (1.0, 'thus'),\n",
       " (1.0, 'terms'),\n",
       " (1.0, 'question'),\n",
       " (1.0, 'provided'),\n",
       " (1.0, 'poverty'),\n",
       " (1.0, 'populating'),\n",
       " (1.0, 'popularized'),\n",
       " (1.0, 'operationalized'),\n",
       " (1.0, 'note'),\n",
       " (1.0, 'looking'),\n",
       " (1.0, 'linguistics'),\n",
       " (1.0, 'keeps'),\n",
       " (1.0, 'high'),\n",
       " (1.0, 'generalizing'),\n",
       " (1.0, 'firth'),\n",
       " (1.0, 'figure'),\n",
       " (1.0, 'e'),\n",
       " (1.0, 'distributions'),\n",
       " (1.0, 'derived'),\n",
       " (1.0, 'dependency'),\n",
       " (1.0, 'data'),\n",
       " (1.0, 'correlation'),\n",
       " (1.0, 'context'),\n",
       " (1.0, 'company'),\n",
       " (1.0, 'children'),\n",
       " (1.0, 'characterized'),\n",
       " (1.0, 'basis'),\n",
       " (1.0, 'although'),\n",
       " (1.0, 'able')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.get_ranked_phrases_with_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_ru = \"\"\"\n",
    "«Дистрибутивный анализ — это метод исследования языка, основанный на изучении окружения (дистрибуции, распределения) отдельных \n",
    "единиц в тексте и не использующий сведений о полном лексическом или грамматическом значении этих единиц».\n",
    "В рамках данного метода к текстам изучаемого языка применяется упорядоченный набор универсальных процедур, \n",
    "что позволяет выделить основные единицы языка (фонемы, морфемы, слова, словосочетания), провести их классификацию и установить \n",
    "отношения сочетаемости между ними.\n",
    "\n",
    "Классификация основывается на принципе замещения: языковые единицы относятся к одному и тому же классу, \n",
    "если они могут выступать в одних и тех же контекстах.\n",
    "\n",
    "Дистрибутивный анализ был предложен Л. Блумфилдом в 20-х гг. XX века и применялся, главным образом, в фонологии и морфологии.\n",
    "3. Харрис и другие представители дескриптивной лингвистики развивали данный метод в своих работах в 30 — 50-х гг. XX века.\n",
    "Близкие идеи выдвигали основоположники структурной лингвистики Ф. де Соссюр и Л. Витгенштейн.\n",
    "Идея контекстных векторов была предложена психологом Ч. Осгудом в рамках работ по представлению значений слов.\n",
    "Контексты, в которых встречались слова, выступали в качестве измерений многоразрядных векторов.\n",
    "В качестве таких контекстов в работах Осгуда использовались антонимические пары прилагательных (например, быстрый-медленный),\n",
    "для которых участники опроса выставляли оценки по семибалльной шкале.\n",
    "\n",
    "В течение последних двух десятилетий метод дистрибутивного анализа широко применялся к изучению семантики.\n",
    "Была разработана дистрибутивно-семантическая методика и соответствующее программное обеспечение, \n",
    "которые позволяют автоматически сравнивать контексты, в которых встречаются изучаемые языковые единицы, \n",
    "и вычислять семантические расстояния между ними.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = rake_nltk.Rake(language='russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(76.0,\n",
       "  'течение последних двух десятилетий метод дистрибутивного анализа широко применялся'),\n",
       " (63.0,\n",
       "  'текстам изучаемого языка применяется упорядоченный набор универсальных процедур'),\n",
       " (54.5, '« дистрибутивный анализ — это метод исследования языка'),\n",
       " (50.0,\n",
       "  'другие представители дескриптивной лингвистики развивали данный метод'),\n",
       " (49.0, 'близкие идеи выдвигали основоположники структурной лингвистики ф'),\n",
       " (34.0, 'работах осгуда использовались антонимические пары прилагательных'),\n",
       " (26.333333333333332, 'позволяет выделить основные единицы языка'),\n",
       " (24.333333333333332, 'которых участники опроса выставляли оценки'),\n",
       " (23.5, 'грамматическом значении этих единиц ».'),\n",
       " (23.0, 'которые позволяют автоматически сравнивать контексты'),\n",
       " (22.666666666666664, 'которых встречаются изучаемые языковые единицы'),\n",
       " (15.0, 'качестве измерений многоразрядных векторов'),\n",
       " (11.5, '30 — 50'),\n",
       " (11.333333333333332, 'языковые единицы относятся'),\n",
       " (10.0, 'дистрибутивный анализ'),\n",
       " (9.5, 'качестве таких контекстов'),\n",
       " (9.5, 'идея контекстных векторов'),\n",
       " (9.333333333333332, 'которых встречались слова'),\n",
       " (9.0, 'установить отношения сочетаемости'),\n",
       " (9.0, 'соответствующее программное обеспечение'),\n",
       " (9.0, 'представлению значений слов'),\n",
       " (9.0, 'предложена психологом ч'),\n",
       " (9.0, 'вычислять семантические расстояния'),\n",
       " (8.5, 'словосочетания ), провести'),\n",
       " (8.5, 'рамках данного метода'),\n",
       " (6.0, 'своих работах'),\n",
       " (5.5, 'отдельных единиц'),\n",
       " (5.0, 'применялся'),\n",
       " (4.5, 'рамках работ'),\n",
       " (4.5, 'медленный ),'),\n",
       " (4.0, 'х гг'),\n",
       " (4.0, 'семибалльной шкале'),\n",
       " (4.0, 'семантическая методика'),\n",
       " (4.0, 'разработана дистрибутивно'),\n",
       " (4.0, 'принципе замещения'),\n",
       " (4.0, 'полном лексическом'),\n",
       " (4.0, 'могут выступать'),\n",
       " (4.0, 'классификация основывается'),\n",
       " (4.0, 'использующий сведений'),\n",
       " (4.0, 'изучению семантики'),\n",
       " (4.0, 'изучении окружения'),\n",
       " (4.0, 'де соссюр'),\n",
       " (4.0, 'главным образом'),\n",
       " (4.0, 'xx века'),\n",
       " (3.5, 'предложен л'),\n",
       " (3.0, 'контексты'),\n",
       " (2.0, 'слова'),\n",
       " (1.5, 'л'),\n",
       " (1.0, 'харрис'),\n",
       " (1.0, 'фонологии'),\n",
       " (1.0, 'фонемы'),\n",
       " (1.0, 'тому'),\n",
       " (1.0, 'тех'),\n",
       " (1.0, 'тексте'),\n",
       " (1.0, 'распределения'),\n",
       " (1.0, 'основанный'),\n",
       " (1.0, 'осгудом'),\n",
       " (1.0, 'одному'),\n",
       " (1.0, 'одних'),\n",
       " (1.0, 'ними'),\n",
       " (1.0, 'например'),\n",
       " (1.0, 'морфологии'),\n",
       " (1.0, 'морфемы'),\n",
       " (1.0, 'контекстах'),\n",
       " (1.0, 'классу'),\n",
       " (1.0, 'классификацию'),\n",
       " (1.0, 'дистрибуции'),\n",
       " (1.0, 'выступали'),\n",
       " (1.0, 'витгенштейн'),\n",
       " (1.0, 'быстрый'),\n",
       " (1.0, 'блумфилдом'),\n",
       " (1.0, '3'),\n",
       " (1.0, '20')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.extract_keywords_from_text(text_ru)\n",
    "r.get_ranked_phrases_with_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "r = rake_nltk.Rake(stopwords=nltk.corpus.stopwords.words('russian'), punctuations=['\"', '«', '»', '(', ')', '.', ',', ':'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(75.66666666666666,\n",
       "  'течение последних двух десятилетий метод дистрибутивного анализа широко применялся'),\n",
       " (62.66666666666667,\n",
       "  'текстам изучаемого языка применяется упорядоченный набор универсальных процедур'),\n",
       " (49.666666666666664,\n",
       "  'другие представители дескриптивной лингвистики развивали данный метод'),\n",
       " (49.0, 'близкие идеи выдвигали основоположники структурной лингвистики ф'),\n",
       " (43.833333333333336, 'дистрибутивный анализ — это метод исследования языка'),\n",
       " (34.0, 'работах осгуда использовались антонимические пары прилагательных'),\n",
       " (33.25, '30 — 50 - х гг'),\n",
       " (26.0, 'позволяет выделить основные единицы языка'),\n",
       " (24.75, 'разработана дистрибутивно - семантическая методика'),\n",
       " (24.333333333333332, 'которых участники опроса выставляли оценки'),\n",
       " (23.5, 'грамматическом значении этих единиц ».'),\n",
       " (23.0, 'которые позволяют автоматически сравнивать контексты'),\n",
       " (22.666666666666664, 'которых встречаются изучаемые языковые единицы'),\n",
       " (18.75, '20 - х гг'),\n",
       " (16.25, 'быстрый - медленный ),'),\n",
       " (15.0, 'качестве измерений многоразрядных векторов'),\n",
       " (11.333333333333332, 'языковые единицы относятся'),\n",
       " (9.5, 'словосочетания ), провести'),\n",
       " (9.5, 'качестве таких контекстов'),\n",
       " (9.5, 'идея контекстных векторов'),\n",
       " (9.333333333333332, 'которых встречались слова'),\n",
       " (9.0, 'установить отношения сочетаемости'),\n",
       " (9.0, 'соответствующее программное обеспечение'),\n",
       " (9.0, 'представлению значений слов'),\n",
       " (9.0, 'предложена психологом ч'),\n",
       " (9.0, 'дистрибутивный анализ'),\n",
       " (9.0, 'вычислять семантические расстояния'),\n",
       " (8.5, 'рамках данного метода'),\n",
       " (6.0, 'своих работах'),\n",
       " (5.5, 'отдельных единиц'),\n",
       " (5.0, 'применялся'),\n",
       " (4.5, 'рамках работ'),\n",
       " (4.0, 'семибалльной шкале'),\n",
       " (4.0, 'принципе замещения'),\n",
       " (4.0, 'полном лексическом'),\n",
       " (4.0, 'могут выступать'),\n",
       " (4.0, 'классификация основывается'),\n",
       " (4.0, 'использующий сведений'),\n",
       " (4.0, 'изучению семантики'),\n",
       " (4.0, 'изучении окружения'),\n",
       " (4.0, 'де соссюр'),\n",
       " (4.0, 'главным образом'),\n",
       " (4.0, 'xx века'),\n",
       " (3.5, 'предложен л'),\n",
       " (3.0, 'контексты'),\n",
       " (2.0, 'слова'),\n",
       " (1.5, 'л'),\n",
       " (1.0, 'харрис'),\n",
       " (1.0, 'фонологии'),\n",
       " (1.0, 'фонемы'),\n",
       " (1.0, 'тому'),\n",
       " (1.0, 'тех'),\n",
       " (1.0, 'тексте'),\n",
       " (1.0, 'распределения'),\n",
       " (1.0, 'основанный'),\n",
       " (1.0, 'осгудом'),\n",
       " (1.0, 'одному'),\n",
       " (1.0, 'одних'),\n",
       " (1.0, 'ними'),\n",
       " (1.0, 'например'),\n",
       " (1.0, 'морфологии'),\n",
       " (1.0, 'морфемы'),\n",
       " (1.0, 'контекстах'),\n",
       " (1.0, 'классу'),\n",
       " (1.0, 'классификацию'),\n",
       " (1.0, 'дистрибуции'),\n",
       " (1.0, 'выступали'),\n",
       " (1.0, 'витгенштейн'),\n",
       " (1.0, 'блумфилдом'),\n",
       " (1.0, '3')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.extract_keywords_from_text(text_ru)\n",
    "r.get_ranked_phrases_with_scores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mystem = pymystem3.Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lemmatized_text = ''.join(mystem.lemmatize(text_ru)).replace('\\n', ' ').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(55.0,\n",
       "  'текст изучать язык применяться упорядочивать набор универсальный процедура'),\n",
       " (52.08333333333333, '« дистрибутивный анализ — это метод исследование язык'),\n",
       " (46.5, 'близкий идея выдвигать основоположник структурный лингвистика ф'),\n",
       " (34.75, 'представитель дескриптивный лингвистика развивать данный метод'),\n",
       " (33.333333333333336,\n",
       "  'работа осгуда использоваться антонимический пара прилагательное'),\n",
       " (32.58333333333333,\n",
       "  'десятилетие метод дистрибутивный анализ широко применяться'),\n",
       " (25.4, 'позволять выделять основной единица язык'),\n",
       " (24.5, 'который участник опрос выставлять оценка'),\n",
       " (22.5, 'который позволять автоматически сравнивать контекст'),\n",
       " (22.4, 'который встречаться изучать языковой единица'),\n",
       " (14.0, 'качество измерение многоразрядный вектор'),\n",
       " (12.75, 'рамка данный метод'),\n",
       " (11.5, 'идея контекстный вектор'),\n",
       " (11.5, '30 — 50'),\n",
       " (10.833333333333334, 'который встречаться слово'),\n",
       " (10.4, 'языковой единица относиться'),\n",
       " (9.833333333333332, 'дистрибутивный анализ'),\n",
       " (9.0, 'устанавливать отношение сочетаемость'),\n",
       " (9.0, 'соответствовать программный обеспечение'),\n",
       " (8.5, 'словосочетание ), проводить'),\n",
       " (8.5, 'предлагать психолог ч'),\n",
       " (8.5, 'вычислять семантический расстояние'),\n",
       " (7.833333333333334, 'представление значение слово'),\n",
       " (6.5, 'разрабатывать дистрибутивный'),\n",
       " (5.833333333333334, 'рамка работа'),\n",
       " (5.4, 'отдельный единица'),\n",
       " (5.4, 'единица ».'),\n",
       " (5.333333333333334, 'свой работа'),\n",
       " (5.0, 'применяться'),\n",
       " (4.5, 'текст'),\n",
       " (4.5, 'семантический методика'),\n",
       " (4.5, 'медленный ),'),\n",
       " (4.5, 'грамматический значение'),\n",
       " (4.0, 'х гг'),\n",
       " (4.0, 'течение последний'),\n",
       " (4.0, 'семибалльный шкала'),\n",
       " (4.0, 'принцип замещение'),\n",
       " (4.0, 'предлагать л'),\n",
       " (4.0, 'полный лексический'),\n",
       " (4.0, 'использовать сведение'),\n",
       " (4.0, 'изучение семантика'),\n",
       " (4.0, 'изучение окружение'),\n",
       " (4.0, 'де соссюр'),\n",
       " (4.0, 'главный образ'),\n",
       " (4.0, 'xx век'),\n",
       " (3.5, 'мочь выступать'),\n",
       " (3.5, 'классификация основываться'),\n",
       " (3.0, 'контекст'),\n",
       " (2.5, 'качество'),\n",
       " (2.3333333333333335, 'слово'),\n",
       " (1.5, 'л'),\n",
       " (1.5, 'классификация'),\n",
       " (1.5, 'выступать'),\n",
       " (1.0, 'харрис'),\n",
       " (1.0, 'фонология'),\n",
       " (1.0, 'фонема'),\n",
       " (1.0, 'распределение'),\n",
       " (1.0, 'основывать'),\n",
       " (1.0, 'осгуд'),\n",
       " (1.0, 'например'),\n",
       " (1.0, 'морфология'),\n",
       " (1.0, 'морфема'),\n",
       " (1.0, 'класс'),\n",
       " (1.0, 'дистрибуция'),\n",
       " (1.0, 'витгенштейн'),\n",
       " (1.0, 'быстрый'),\n",
       " (1.0, 'блумфилд'),\n",
       " (1.0, '3'),\n",
       " (1.0, '20')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = rake_nltk.Rake(language='russian')\n",
    "r.extract_keywords_from_text(lemmatized_text)\n",
    "r.get_ranked_phrases_with_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. TF.IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_top_idf_features(tfidf, top_n):\n",
    "    indices = np.argsort(tfidf.idf_)[::-1]\n",
    "    features = tfidf.get_feature_names()\n",
    "    top_features = [features[i] for i in indices[:top_n]]\n",
    "\n",
    "    return top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x147 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 147 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=stopwords.words('russian'))\n",
    "tfidf.fit_transform([text_ru])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['языковые',\n",
       " 'единиц',\n",
       " 'классификацию',\n",
       " 'качестве',\n",
       " 'исследования',\n",
       " 'использующий',\n",
       " 'использовались',\n",
       " 'изучению',\n",
       " 'изучении',\n",
       " 'изучаемые']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_idf_features(tfidf, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x480 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 480 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=stopwords.words('russian'), ngram_range=(1,3))\n",
    "tfidf.fit_transform([text_ru])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['языковые единицы относятся',\n",
       " 'языковые единицы вычислять',\n",
       " 'изучаемые языковые единицы',\n",
       " 'изучении',\n",
       " 'изучении окружения',\n",
       " 'изучении окружения дистрибуции',\n",
       " 'изучению',\n",
       " 'изучению семантики',\n",
       " 'изучению семантики разработана',\n",
       " 'использовались']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_idf_features(tfidf, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем взять лемматизированный текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x115 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 115 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=stopwords.words('russian'))\n",
    "tfidf.fit_transform([lemmatized_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['языковой',\n",
       " 'единица',\n",
       " 'значение',\n",
       " 'идея',\n",
       " 'измерение',\n",
       " 'изучать',\n",
       " 'изучение',\n",
       " 'использовать',\n",
       " 'использоваться',\n",
       " 'исследование']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_idf_features(tfidf, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x424 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 424 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=stopwords.words('russian'), ngram_range=(1,3))\n",
    "tfidf.fit_transform([lemmatized_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['языковой единица относиться',\n",
       " 'изучать языковой',\n",
       " 'изучение',\n",
       " 'изучение окружение',\n",
       " 'изучение окружение дистрибуция',\n",
       " 'изучение семантика',\n",
       " 'изучение семантика разрабатывать',\n",
       " 'использовать',\n",
       " 'использовать сведение',\n",
       " 'использовать сведение полный']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_idf_features(tfidf, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что будет, если взять больше текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.datasets\n",
    "news = sklearn.datasets.fetch_20newsgroups(subset='train', categories=['comp.graphics', 'sci.med'], shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1178x298186 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 504063 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=stopwords.words('english'), ngram_range=(1,3))\n",
    "tfidf.fit_transform(news.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['íålittin less 92',\n",
       " 'framingham mwra',\n",
       " 'franceus ultimage',\n",
       " 'franceus',\n",
       " 'frances nixon 1973',\n",
       " 'frances nixon',\n",
       " 'frances',\n",
       " 'france uk tga',\n",
       " 'france uk',\n",
       " 'france package received',\n",
       " 'france package',\n",
       " 'france lines significant',\n",
       " 'france lines 45',\n",
       " 'france lines 32',\n",
       " 'france lines 106',\n",
       " 'france internet minitel',\n",
       " 'france internet',\n",
       " 'france free listed',\n",
       " 'france free',\n",
       " 'france family image',\n",
       " 'france family',\n",
       " 'france commercial program',\n",
       " 'france commercial',\n",
       " 'fran nevada reno',\n",
       " 'fran nevada',\n",
       " 'fran',\n",
       " 'framming animation raytracer',\n",
       " 'framming animation',\n",
       " 'framming',\n",
       " 'framingham water div',\n",
       " 'framingham water',\n",
       " 'franceus ultimage concept',\n",
       " 'francis heylighen',\n",
       " 'francis heylighen hin9',\n",
       " 'francisco biocine joint',\n",
       " 'francisco general',\n",
       " 'francisco filter remove',\n",
       " 'francisco filter',\n",
       " 'francisco distribution usa',\n",
       " 'francisco distribution',\n",
       " 'francisco dejesus dejesus',\n",
       " 'francisco dejesus',\n",
       " 'francisco city county',\n",
       " 'francisco city',\n",
       " 'francisco chris fields',\n",
       " 'francisco chris',\n",
       " 'francisco california usa',\n",
       " 'francisco california',\n",
       " 'francisco biocine',\n",
       " 'francis marchese',\n",
       " 'francisco bay area',\n",
       " 'francisco bay',\n",
       " 'francisco anyone information',\n",
       " 'francisco anyone',\n",
       " 'francisco also recent',\n",
       " 'francisco also',\n",
       " 'francis writes article',\n",
       " 'francis writes',\n",
       " 'francis uy werner',\n",
       " 'francis uy',\n",
       " 'francis subject wear',\n",
       " 'francis subject krillean',\n",
       " 'francis marchese computer',\n",
       " 'framingham mwra framingham',\n",
       " 'framingham',\n",
       " 'francisco lines 24',\n",
       " 'framework supposedly fairly',\n",
       " 'frames eti set',\n",
       " 'frames eti',\n",
       " 'frames 512 512',\n",
       " 'frames 512',\n",
       " 'framemaker mif files',\n",
       " 'framemaker mif',\n",
       " 'framemaker',\n",
       " 'framegrabber board standard',\n",
       " 'framegrabber board',\n",
       " 'framegrabber 16bit yesterday',\n",
       " 'framegrabber 16bit',\n",
       " 'framed area plot',\n",
       " 'framed area',\n",
       " 'framed',\n",
       " 'frame would lead',\n",
       " 'frame would',\n",
       " 'frame time frame',\n",
       " 'frame time',\n",
       " 'frame tape play',\n",
       " 'frame tape',\n",
       " 'frame stores put',\n",
       " 'frame stores',\n",
       " 'frame size least',\n",
       " 'frame size',\n",
       " 'frame rates rendering',\n",
       " 'frame rates mouse',\n",
       " 'frame rate version',\n",
       " 'frame rate system',\n",
       " 'frame rate must',\n",
       " 'frames filter',\n",
       " 'frames filter image',\n",
       " 'frames first',\n",
       " 'frames sometimes']"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_idf_features(tfidf, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+git://github.com/davidadamojr/TextRank.git\n",
      "  Cloning git://github.com/davidadamojr/TextRank.git to /private/var/folders/rs/q24l0yj910g830qbcn9j6k6m0000gp/T/pip-Vaq6ps-build\n",
      "Requirement already satisfied: networkx>=1.11.0 in /Users/max/anaconda/lib/python2.7/site-packages (from textrank==0.1.0)\n",
      "Requirement already satisfied: nltk>=3.2.1 in /Users/max/anaconda/lib/python2.7/site-packages (from textrank==0.1.0)\n",
      "Requirement already satisfied: numpy>=1.11.2 in /Users/max/anaconda/lib/python2.7/site-packages (from textrank==0.1.0)\n",
      "Requirement already satisfied: click>=6.6 in /Users/max/anaconda/lib/python2.7/site-packages (from textrank==0.1.0)\n",
      "Requirement already satisfied: decorator>=3.4.0 in /Users/max/anaconda/lib/python2.7/site-packages (from networkx>=1.11.0->textrank==0.1.0)\n",
      "Installing collected packages: textrank\n",
      "  Running setup.py install for textrank ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
      "\u001b[?25hSuccessfully installed textrank-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+git://github.com/davidadamojr/TextRank.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Different',\n",
       " 'Distributional Hypothesis',\n",
       " 'Distributional hypothesis',\n",
       " 'HAL',\n",
       " 'Hyperspace Analogue',\n",
       " 'LSA',\n",
       " 'Language',\n",
       " 'analysis',\n",
       " 'approach',\n",
       " 'attention',\n",
       " 'co-occur',\n",
       " 'cognitive',\n",
       " 'computational modeling',\n",
       " 'contexts',\n",
       " 'correlation',\n",
       " 'data-sparsity problem',\n",
       " 'dependency-based',\n",
       " 'different',\n",
       " 'distributional hypothesis',\n",
       " 'distributional information',\n",
       " 'distributional/semantic similarity',\n",
       " 'high-dimensional',\n",
       " 'impoverished',\n",
       " 'indexing',\n",
       " 'individual',\n",
       " 'language',\n",
       " 'language learning',\n",
       " 'linguistic contexts',\n",
       " 'modeling Distributional',\n",
       " 'paradigmatic',\n",
       " 'question',\n",
       " 'representational framework',\n",
       " 'significant',\n",
       " 'similarity-based generalization',\n",
       " 'statistical',\n",
       " 'stimulus',\n",
       " 'suggestion',\n",
       " 'syntagmatic'}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textrank.extract_key_phrases(text_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Different kinds of similarities can be extracted depending on which type of distributional information is used to collect the vectors: topical similarities can be extracted by populating the vectors with information on which text regions the linguistic items occur in; paradigmatic similarities can be extracted by populating the vectors with information on which other linguistic items the items co-occur with. Whether or not this suggestion holds has significant implications for both the data-sparsity problem in computational modeling, and for the question of how children are able to learn language so rapidly given relatively impoverished input (this is also known as'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textrank.extract_sentences(text_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для русского это не работает, потому что под капотом используется тэггер NLTK для английского."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Блумфилдом',\n",
       " 'Витгенштейн',\n",
       " 'Дистрибутивный',\n",
       " 'Контексты',\n",
       " 'автоматически сравнивать',\n",
       " 'быстрый-медленный',\n",
       " 'встречались',\n",
       " 'встречаются изучаемые',\n",
       " 'выдвигали основоположники',\n",
       " 'выставляли',\n",
       " 'выступали',\n",
       " 'выступать',\n",
       " 'вычислять семантические',\n",
       " 'грамматическом',\n",
       " 'дескриптивной лингвистики',\n",
       " 'десятилетий',\n",
       " 'дистрибутивного',\n",
       " 'дистрибуции',\n",
       " 'замещения',\n",
       " 'измерений многоразрядных',\n",
       " 'изучаемого',\n",
       " 'изучаемые языковые',\n",
       " 'использовались антонимические',\n",
       " 'использующий',\n",
       " 'исследования',\n",
       " 'классификацию',\n",
       " 'контекстах',\n",
       " 'контекстных',\n",
       " 'контекстов',\n",
       " 'лексическом',\n",
       " 'лингвистики развивали',\n",
       " 'морфологии',\n",
       " 'окружения',\n",
       " 'основанный',\n",
       " 'основоположники структурной',\n",
       " 'основывается',\n",
       " 'относятся',\n",
       " 'позволяет',\n",
       " 'позволяют автоматически',\n",
       " 'предложен',\n",
       " 'предложена психологом',\n",
       " 'представители дескриптивной',\n",
       " 'представлению',\n",
       " 'прилагательных',\n",
       " 'применяется упорядоченный',\n",
       " 'применялся',\n",
       " 'программное обеспечение',\n",
       " 'разработана дистрибутивно-семантическая',\n",
       " 'распределения',\n",
       " 'семантики',\n",
       " 'семантические расстояния',\n",
       " 'семибалльной',\n",
       " 'словосочетания',\n",
       " 'соответствующее программное',\n",
       " 'сочетаемости',\n",
       " 'сравнивать контексты',\n",
       " 'структурной лингвистики',\n",
       " 'универсальных',\n",
       " 'установить',\n",
       " 'участники',\n",
       " 'фонологии',\n",
       " 'языковые'}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textrank.extract_key_phrases(text_ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для русского вместо этого можно использовать правиловый извлекатель `termextract`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rutermextract\n",
      "  Using cached rutermextract-0.2.zip\n",
      "Collecting pymorphy2>=0.8 (from rutermextract)\n",
      "  Downloading pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 490kB/s \n",
      "\u001b[?25hRequirement already satisfied: enum34>=1.0 in /Users/max/anaconda/lib/python2.7/site-packages (from rutermextract)\n",
      "Collecting docopt>=0.6 (from pymorphy2>=0.8->rutermextract)\n",
      "  Downloading docopt-0.6.2.tar.gz\n",
      "Collecting dawg-python>=0.7 (from pymorphy2>=0.8->rutermextract)\n",
      "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
      "Collecting pymorphy2-dicts<3.0,>=2.4 (from pymorphy2>=0.8->rutermextract)\n",
      "  Downloading pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 7.1MB 45kB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: rutermextract, docopt\n",
      "  Running setup.py bdist_wheel for rutermextract ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/max/Library/Caches/pip/wheels/a6/39/49/689b12e89530ced3cf0dd0b0b01ffe6aacc3d79da7522c5613\n",
      "  Running setup.py bdist_wheel for docopt ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n",
      "\u001b[?25h  Stored in directory: /Users/max/Library/Caches/pip/wheels/b2/16/5f/c33a2bb5f2dce71205f8e65cbfd05647d79d441282be31fd82\n",
      "Successfully built rutermextract docopt\n",
      "Installing collected packages: docopt, dawg-python, pymorphy2-dicts, pymorphy2, rutermextract\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 rutermextract-0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install rutermextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rutermextract import TermExtractor\n",
    "\n",
    "term_extractor = TermExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/max/anaconda/envs/py36/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "дистрибутивный анализ 3\n",
      "контексты 3\n",
      "данный метод 2\n",
      "слово 2\n",
      "л 2\n",
      "классификация 2\n",
      "гг 2\n",
      "век 2\n",
      "соответствующее программное обеспечение 1\n",
      "представление значений слов 1\n",
      "последние два десятилетий 1\n",
      "основные единица языка 1\n",
      "метод исследования языка 1\n",
      "изучаемые языковые единица 1\n",
      "антонимические пара прилагательных 1\n",
      "языковые единица 1\n",
      "участники опроса 1\n",
      "упорядоченный набор 1\n",
      "универсальные процедуры 1\n",
      "такие контексты 1\n",
      "структурная лингвистика 1\n",
      "семибалльная шкала 1\n",
      "семантические расстояние 1\n",
      "рамки работ 1\n",
      "работы осгуда 1\n",
      "принцип замещения 1\n",
      "отношение сочетаемости 1\n",
      "отдельные единицы 1\n",
      "многоразрядные векторы 1\n",
      "контекстные векторы 1\n",
      "качество измерений 1\n",
      "использующий сведения 1\n",
      "изучение семантики 1\n",
      "изучение окружения 1\n",
      "изучаемый язык 1\n",
      "другие представители 1\n",
      "дистрибутивно-семантическая методика 1\n",
      "дескриптивная лингвистика 1\n",
      "грамматический значение 1\n",
      "близкие идеи 1\n",
      "число 1\n",
      "харрис 1\n",
      "фонология 1\n",
      "фонема 1\n",
      "ф 1\n",
      "течение 1\n",
      "тексты 1\n",
      "текст 1\n",
      "соссюры 1\n",
      "словосочетание 1\n",
      "распределение 1\n",
      "рамки 1\n",
      "работы 1\n",
      "психолог 1\n",
      "оценка 1\n",
      "основоположники 1\n",
      "осгуд 1\n",
      "образ 1\n",
      "морфология 1\n",
      "морфема 1\n",
      "метод 1\n",
      "класс 1\n",
      "качество 1\n",
      "идея 1\n",
      "единицы 1\n",
      "дистрибуция 1\n",
      "главное 1\n",
      "витгенштейн 1\n",
      "блумфилд 1\n",
      "50-хи 1\n",
      "20-хи 1\n"
     ]
    }
   ],
   "source": [
    "for term in term_extractor(text_ru):\n",
    "    print (term.normalized, term.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Извлечение данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. КС-грамматики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample_grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "PP -> P NP\n",
    "NP -> A N | N\n",
    "VP -> V NP | VP PP\n",
    "N -> 'пони' | 'кофе' | 'кенгуру'\n",
    "V -> 'любит' | 'ест'\n",
    "P -> 'с'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = u'пони ест кофе'.split()\n",
    "parser = nltk.ChartParser(sample_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trees = parser.parse_one(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP (N пони))\n",
      "(VP (V ест) (NP (N кофе)))\n"
     ]
    }
   ],
   "source": [
    "for tree in trees:\n",
    "    print (tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nltk.draw.draw_trees(*trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_grammar = nltk.data.load('file:sample_grammar.cfg', cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 15 productions (start state = S)\n",
      "    S -> NP VP\n",
      "    PP -> P NP\n",
      "    NP -> A N\n",
      "    NP -> N\n",
      "    NP -> NP Conj NP\n",
      "    VP -> V NP\n",
      "    VP -> VP PP\n",
      "    N -> 'пони'\n",
      "    N -> 'кофе'\n",
      "    N -> 'кенгуру'\n",
      "    V -> 'любит'\n",
      "    V -> 'ест'\n",
      "    P -> 'с'\n",
      "    Conj -> 'и'\n",
      "    Conj -> 'или'\n"
     ]
    }
   ],
   "source": [
    "print (sample_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parser = nltk.ChartParser(sample_grammar, trace=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|.  пони . любит .  кофе .   и   .кенгуру.|\n",
      "|[-------]       .       .       .       .| [0:1] 'пони'\n",
      "|.       [-------]       .       .       .| [1:2] 'любит'\n",
      "|.       .       [-------]       .       .| [2:3] 'кофе'\n",
      "|.       .       .       [-------]       .| [3:4] 'и'\n",
      "|.       .       .       .       [-------]| [4:5] 'кенгуру'\n",
      "|[-------]       .       .       .       .| [0:1] N  -> 'пони' *\n",
      "|[-------]       .       .       .       .| [0:1] NP -> N *\n",
      "|[------->       .       .       .       .| [0:1] S  -> NP * VP\n",
      "|[------->       .       .       .       .| [0:1] NP -> NP * Conj NP\n",
      "|.       [-------]       .       .       .| [1:2] V  -> 'любит' *\n",
      "|.       [------->       .       .       .| [1:2] VP -> V * NP\n",
      "|.       .       [-------]       .       .| [2:3] N  -> 'кофе' *\n",
      "|.       .       [-------]       .       .| [2:3] NP -> N *\n",
      "|.       .       [------->       .       .| [2:3] S  -> NP * VP\n",
      "|.       .       [------->       .       .| [2:3] NP -> NP * Conj NP\n",
      "|.       [---------------]       .       .| [1:3] VP -> V NP *\n",
      "|.       [--------------->       .       .| [1:3] VP -> VP * PP\n",
      "|[-----------------------]       .       .| [0:3] S  -> NP VP *\n",
      "|.       .       .       [-------]       .| [3:4] Conj -> 'и' *\n",
      "|.       .       [--------------->       .| [2:4] NP -> NP Conj * NP\n",
      "|.       .       .       .       [-------]| [4:5] N  -> 'кенгуру' *\n",
      "|.       .       .       .       [-------]| [4:5] NP -> N *\n",
      "|.       .       .       .       [------->| [4:5] S  -> NP * VP\n",
      "|.       .       .       .       [------->| [4:5] NP -> NP * Conj NP\n",
      "|.       .       [-----------------------]| [2:5] NP -> NP Conj NP *\n",
      "|.       .       [----------------------->| [2:5] S  -> NP * VP\n",
      "|.       .       [----------------------->| [2:5] NP -> NP * Conj NP\n",
      "|.       [-------------------------------]| [1:5] VP -> V NP *\n",
      "|.       [------------------------------->| [1:5] VP -> VP * PP\n",
      "|[=======================================]| [0:5] S  -> NP VP *\n"
     ]
    }
   ],
   "source": [
    "sent = 'пони любит кофе и кенгуру'.split()\n",
    "trees = parser.parse_one(sent)\n",
    "nltk.draw.draw_trees(*trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. GLRParser\n",
    "\n",
    "`GLRParser` — открытый и более доступный аналог Томита-парсера от Яндекса нужно скачать с гитхаба (не устанавливается как пакет):\n",
    "https://github.com/vas3k/python-glr-parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('GLRParser')\n",
    "\n",
    "from glr import GLRParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOUND: 10 апреля\n"
     ]
    }
   ],
   "source": [
    "dictionaries = {\n",
    "    u\"MONTHS\": [u\"январь\", u\"февраль\", u\"март\", u\"апрель\", u\"май\", u\"июнь\", u\"июль\", u\"август\"],\n",
    "}\n",
    "\n",
    "grammar = u\"\"\"\n",
    "    S = word<regex=[0-9]+> MONTHS\n",
    "\"\"\"\n",
    "\n",
    "glr = GLRParser(grammar, dictionaries=dictionaries)\n",
    "\n",
    "text = u\"вылет 10 апреля\"\n",
    "for parsed in glr.parse(text):\n",
    "    print \"FOUND:\", parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOUND: красивый дом\n"
     ]
    }
   ],
   "source": [
    "grammar = u\"\"\"\n",
    "    S = adj<agr-gnc=1> noun\n",
    "\"\"\"\n",
    "\n",
    "glr = GLRParser(grammar)\n",
    "\n",
    "text = u\"большой красивый дом\"\n",
    "for parsed in glr.parse(text):\n",
    "    print \"FOUND:\", parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOUND: красивый дом\n",
      "            \n",
      "FOUND: дом\n",
      "            \n",
      "FOUND: красный красивый дом\n",
      "            \n",
      "FOUND: красивый дом\n",
      "            \n",
      "FOUND: дом\n",
      "            \n",
      "FOUND: красный дом\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "grammar = u\"\"\"\n",
    "    S = NP\n",
    "    NP = adj<agr-gnc=1> NP\n",
    "    NP = noun\n",
    "\"\"\"\n",
    "\n",
    "glr = GLRParser(grammar)\n",
    "\n",
    "text = u\"\"\"большой красивый дом\n",
    "            красный красивый дом\n",
    "            большой красный дом\n",
    "        \"\"\"\n",
    "for parsed in glr.parse(text):\n",
    "    print \"FOUND:\", parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишите грамматику для поиска групп вида \"большой и красный дом\", \"большой дом и красное яблоко\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOUND: красное яблоко\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "grammar = u\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "glr = GLRParser(grammar)\n",
    "\n",
    "text = u\"\"\"большой красивый дом\n",
    "            красный красивый дом\n",
    "            большой красный дом\n",
    "            красивый и красный дом\n",
    "            большой дом и красное яблоко\n",
    "        \"\"\"\n",
    "for parsed in glr.parse(text):\n",
    "    print \"FOUND:\", parsed"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
